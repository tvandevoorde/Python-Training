# Lesson 1.2.36: Optimization Intuition

**Estimated Time**: 8 hours | **Difficulty**: Intermediate  
**Prerequisites**: Lesson 1.2.35 | **Topic**: Calculus for ML

---

## Learning Objectives

By the end of this lesson, you will be able to:
- Describe minima, maxima, and saddle points
- Explain local vs global optima
- Relate gradients to optimization
- Interpret optimization landscapes

---

## Core Ideas

- Optima occur where gradient is zero
- Curvature determines type of critical point

---

## Exercises

### Difficulty 1: Starter
- Exercise 1.2.36.1: Define local minimum.
- Exercise 1.2.36.2: Explain saddle point in words.
- Exercise 1.2.36.3: Describe global minimum.

### Difficulty 2: Intermediate
- Exercise 1.2.36.4: Classify critical points using second derivative.
- Exercise 1.2.36.5: Sketch a simple non-convex function.
- Exercise 1.2.36.6: Explain why optimization can get stuck.

### Difficulty 3: Advanced
- Exercise 1.2.36.7: Discuss convex vs non-convex loss.
- Exercise 1.2.36.8: Explain how initialization affects optimization.
- Exercise 1.2.36.9: Connect Hessian signs to saddle points.

---

## Mini-Project: Landscape Notes

Write a short report comparing convex and non-convex landscapes.

---

## Quiz & Assessment

### Quick Check (True/False)
1. Gradients are zero at critical points. (True)
2. Saddle points are always maxima. (False)
3. Convex functions have one global minimum. (True)
4. Non-convex landscapes can have many minima. (True)

### Conceptual Questions
5. Why are saddle points a problem in ML?
6. How does curvature influence optimization?
7. What makes a function convex?

### Coding Challenge
8. Give an example of a convex and a non-convex function.

---

## Key Takeaways

- Optimization seeks minima of loss
- Gradients and curvature classify points
- Convexity simplifies learning
- Non-convexity adds complexity

---

## Additional Resources

- https://www.khanacademy.org/math/multivariable-calculus
- https://www.deeplearning.ai
- https://www.statquest.org

---

## Next Lesson Preview

**Lesson 1.2.37**: Gradient Descent Basics
- Implement the core algorithm

---

## Homework

- [ ] Complete exercises 1.2.36.1 - 1.2.36.9
- [ ] Sketch three loss landscapes
- [ ] Review convexity concepts

**Expected time**: 2-3 hours

---

**Lesson Complete.**

Proceed to **Lesson 1.2.37** when ready.
