# Lesson 1.2.44: MILESTONE TEST - Calculus and Optimization

**Estimated Time**: 8 hours | **Difficulty**: Intermediate  
**Prerequisites**: Lessons 1.2.26 - 1.2.43 | **Topic**: Assessment - Calculus

---

## Overview

This milestone test verifies mastery of calculus and optimization basics.

**Passing Grade**: 80%  
**Time Limit**: 90 minutes  
**Format**: 40 questions total

---

## Part A: Multiple Choice (10 questions)

1. What does a derivative measure?
2. What is the gradient?
3. When does gradient descent converge?
4. Which rule handles composition?
5. What does Hessian represent?
6. What is a learning rate?
7. What is a partial derivative?
8. What is a convex function?
9. What does regularization do?
10. What is a saddle point?

---

## Part B: True/False (10 questions)

1. Chain rule applies to compositions.
2. Gradients always point downhill.
3. Learning rate too large can diverge.
4. Numerical differentiation is exact.
5. Autodiff uses chain rule.
6. Hessian stores second derivatives.
7. L1 encourages sparsity.
8. Convex losses have one global minimum.
9. Partial derivatives hold others constant.
10. Backprop is reverse-mode autodiff.

---

## Part C: Short Answer (8 questions)

1. Define derivative.
2. Explain gradient descent update.
3. State chain rule.
4. Describe Jacobian.
5. Define Hessian.
6. Explain learning rate effect.
7. Define regularization.
8. Describe numerical differentiation.

---

## Part D: Coding Problems (2 questions)

### Problem 1: Gradient Descent
Implement gradient descent for f(x)=x^2.

### Problem 2: Gradient Check
Compare numeric and analytic gradients for f(x)=x^2.

---

## Scoring Guide

- Multiple Choice: 20 points
- True/False: 10 points
- Short Answer: 20 points
- Coding: 10 points

---

## Next Steps

Review weak topics and retake if below 80%.

---

**Milestone Complete.**

Proceed to **Lesson 1.2.45** when ready.
